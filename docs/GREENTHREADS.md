# Green Threads and GTK Integration Plan

This document outlines the implementation of cooperative green processes (green threads) and the planned GTK4 integration for Nemo. The design centers around Monitors as the fundamental concurrency primitive, with shared heaps and copy-on-pass by reference.

**Status**: Phase 1 (Core Scheduler) âœ… Implemented | Phase 2 (GTK Integration) ðŸš§ Planned

**See also:**
- [Concurrency Design](./CONCURRENCY.md) â€“ background theory and comparisons.
- [Gtk Intro](./GINTRO.md) â€“ GTK4 Nim bindings.
- [API Reference](#api-reference) â€“ Nemo-side API documentation.

---

## 1. What we are building

We implement a **userâ€‘space cooperative scheduler** for _green processes_, each with its own Nemo interpreter and activation stack. The scheduler runs all processes in a single Nim thread, sharing a common heap but with perâ€‘process activation stacks.

A **Monitor** is the fundamental coordination primitive, providing reâ€‘entrant mutual exclusion and condition variables. All other IPC (SharedQueue, Channel) is built on Monitors.

A GTKâ€‘idle callback runs the scheduler; when the readyâ€‘queue is empty, we yield to the GTK event loop, so GTK events are processed with soft realâ€‘time responsiveness.

---

## 2. Core Concepts

### 2.1. Greenâ€‘process model (coâ€‘operative)

- **Singleâ€‘threaded** Nemo VM (one OS thread, green processes scheduled cooperatively).
- **Explicit yield points** on `Processor yield`, when blocking on a synchronization primitive, or optionally after N message sends (configurable via `yieldEveryNSends`).
- Processes run in a **sharedâ€‘heap, privateâ€‘stack** model: the Nemo heap is common, but each process has its own activation stack and programâ€‘counterâ€‘like â€œinstruction pointerâ€.
- Communication uses Monitors, builtâ€‘onâ€‘them SharedQueues, and later channels (bounded buffers). No copying of objects passed via a channel (shared heap, passâ€‘byâ€‘reference).
- **Blocking**: when a process would block on a Monitor, SharedQueue, or Channel, the scheduler moves it to a blocked set.

### 2.2. Synchronisation & Communication Primitives

Primitives in order of increasing abstraction:

1. **Monitor (core)** â€“ reâ€‘entrant lock + condition variables (`wait`, `notify`, `notifyAll`).
   - For single-threaded cooperative scheduling, monitors are pure bookkeeping (no OS locks needed).
   - Reâ€‘entrancy is perâ€‘process: the same green thread can lock a monitor it already holds.

2. **Semaphore (binary/counting)** â€“ built on Monitor + an integer count. Classic `signal`/`wait`.

3. **SharedQueue (bounded)** â€“ producerâ€‘consumer queue built on a Monitor, with a fixedâ€‘capacity array and two condition variables (`nonâ€‘empty`, `nonâ€‘full`). This is the primary IPC primitive.

4. **Channel (CSP style, bounded buffer)** â€“ built on SharedQueue, typed messages.

5. **Actorâ€‘style mailbox** â€“ each process can have a mailbox (SharedQueue) that receives messages as (selectorâ€‘string, argumentâ€‘list).

### 2.3. Communication Primitives (Nimâ€‘side Naming)

| Primitive           | Nimâ€‘side type        | Nemoâ€‘side class | Purpose                               |
|--------------------|---------------------|----------------------|---------------------------------------|
| Monitor            | `NemoMonitor`     | `Monitor`            | Reâ€‘entrant mutex + condition vars   |
| Semaphore          | `NemoSemaphore`  | `Semaphore`          | Binary/counting semaphore            |
| SharedQueue        | `NemoSharedQueue` | `SharedQueue`        | Classic Smalltalk bounded queue      |
| Channel           | `NemoChannel`     | `Channel` (bounded)  | CSPâ€‘style typed channel (future)    |

All primitives are designed to work purely cooperatively: when a green process blocks (waiting on a monitor or queue), the scheduler puts it aside and runs the next process.

### 2.4. Debugging & Inspection

Because each process has its own interpreter and activation stack, a debugger written in Nemo can:

1. Attach to any process (suspend it).
2. Walk its stack frames (each frame has `receiver`, `method`, locals, PC).
3. Step the process one Nemo messageâ€‘send at a time.

The debugger is just another Nemo process, using the same APIs that the Nim scheduler provides.

---

## 3. Implementation

### 3.1. Nimâ€‘side Data Structures

```nim
type
  ProcessState* = enum
    psReady, psRunning, psBlocked, psSuspended, psTerminated

  WaitKind* = enum
    wkMonitor, wkSemaphore, wkQueueFull, wkQueueEmpty

  WaitCondition* = object
    kind*: WaitKind
    target*: pointer  # The monitor/queue being waited on

  Process* = ref object
    pid: uint64
    name: string
    interpreter: Interpreter  # Each process has its own interpreter
    state: ProcessState
    blockedOn: WaitCondition  # Valid when state == psBlocked
    priority: int             # Higher = runs first (0 = normal)
```

**Process/Interpreter Relationship:**
- Each `Process` contains its own `Interpreter` instance with its own activation stack
- The activation stack and program counter live inside the `Interpreter`, not duplicated in `Process`
- All interpreters share a common `globals` table (for global variables)
- All interpreters share a common `rootObject` (the class hierarchy root)

### 3.2. Greenâ€‘threads Scheduler

One global scheduler holds:

- `readyQueue: Deque[Process]`
- `blockedSet: Set[Process]`
- `allProcesses: Table[uint64, Process]`

Scheduler loop:

1. If `readyQueue` empty â†’ remove GTK idle callback (nothing to do).
2. Pop first process from `readyQueue`.
3. Switch context (current interpreter = its interpreter; set current process).
4. Let the interpreter run **one Nemo message send** (or a timeâ€‘slice of byteâ€‘code).
5. If the process didn't block, push back to tail of readyQueue.
6. If it blocked on a condition, move it to the appropriate blocked set.
7. Goto step 1.

**GTK Idle Callback Strategy**:
- Install an idle callback when processes become ready (e.g., after spawning or unblocking).
- Remove the callback when the ready queue becomes empty.
- Each callback invocation runs one process slice, then returns control to GTK.
- This ensures GTK events are processed with soft real-time responsiveness.

```nim
proc schedulerIdleCallback(): bool =
  ## Returns true to keep callback installed, false to remove it
  if sched.readyQueue.len == 0:
    return false  # Remove callback, nothing to run
  sched.runOneSlice()
  return true  # Keep callback installed
```

### 3.3. GTK Integration

Each Nemoâ€‘side GTK widget holds a Nimâ€‘side pointer to the underlying GTK widget and a finalizer.

When a GTK event occurs (buttonâ€‘click), the GTK main loop invokes our Nimâ€‘side callback, which does:

```nim
let process = processOwningThisWidget(proc)
var callbackBlock: BlockNode
# ... wrap GTK arguments into Nemo objects, push a new activation frame
pushActivation(proc, callbackBlock, args)
schedule(proc)
```

The callback is executed next time the associated Nemo process is scheduled.

### 3.4. Monitor Implementation

For single-threaded cooperative scheduling, monitors are just bookkeepingâ€”no OS-level locks are needed since only one green process runs at a time:

```nim
type
  NemoMonitor* = ref object
    owner*: Process           # Which process holds the lock (nil = unlocked)
    lockCount*: int           # Re-entrancy count
    waitQueue*: seq[Process]  # Processes waiting to acquire

proc enter*(m: NemoMonitor, sched: Scheduler, caller: Process) =
  if m.owner == nil:
    m.owner = caller
    m.lockCount = 1
  elif m.owner == caller:
    inc m.lockCount  # Re-entrant
  else:
    m.waitQueue.add(caller)
    sched.block(caller, WaitCondition(kind: wkMonitor, target: cast[pointer](m)))

proc leave*(m: NemoMonitor, sched: Scheduler) =
  dec m.lockCount
  if m.lockCount == 0:
    m.owner = nil
    if m.waitQueue.len > 0:
      let next = m.waitQueue.pop()
      m.owner = next
      m.lockCount = 1
      sched.unblock(next)
```

### 3.5. SharedQueue (classic bounded buffer)

A bounded buffer with separate wait queues for producers and consumers:

```nim
type
  NemoSharedQueue*[T] = ref object
    data: seq[T]
    head, tail, count: int
    capacity: int
    waitingProducers: seq[Process]  # Blocked on "queue full"
    waitingConsumers: seq[Process]  # Blocked on "queue empty"

proc put*(q: NemoSharedQueue, sched: Scheduler, item: NodeValue, caller: Process) =
  if q.count == q.capacity:
    q.waitingProducers.add(caller)
    sched.block(caller, WaitCondition(kind: wkQueueFull, target: cast[pointer](q)))
    return

  q.data[q.tail] = item
  q.tail = (q.tail + 1) mod q.capacity
  inc q.count

  if q.waitingConsumers.len > 0:
    sched.unblock(q.waitingConsumers.pop())

proc take*(q: NemoSharedQueue, sched: Scheduler, caller: Process): NodeValue =
  if q.count == 0:
    q.waitingConsumers.add(caller)
    sched.block(caller, WaitCondition(kind: wkQueueEmpty, target: cast[pointer](q)))
    return nil  # Will resume with result when unblocked

  result = q.data[q.head]
  q.head = (q.head + 1) mod q.capacity
  dec q.count

  if q.waitingProducers.len > 0:
    sched.unblock(q.waitingProducers.pop())
```

### 3.6. Process Spawning Mechanics

When a Nemo program calls `Processor fork: aBlock`, the following Nim-side code runs:

```nim
proc forkProcess*(sched: var Scheduler, block: BlockNode, receiver: Instance): Process =
  ## Create a new green process from a Nemo block
  let newInterp = newInterpreter()
  newInterp.globals = sched.sharedGlobals     # Share globals table
  newInterp.rootObject = sched.rootObject     "Share class root

  result = Process(
    pid: sched.nextPid(),
    interpreter: newInterp,
    state: psReady
  )

  # Set up initial activation frame for the block
  let activation = newActivation(receiver, block)
  newInterp.activationStack.add(activation)
  newInterp.currentActivation = activation

  # Add to scheduler
  sched.readyQueue.addLast(result)
  sched.allProcesses[result.pid] = result
```

### 3.7. Nemo API Examples

Here's what the Nemo programmer sees:

```smalltalk
"Fork a new process"
process := Processor fork: [
  1 to: 10 do: [:i | Transcript show: i]
]

"Explicit yield to other processes"
Processor yield

"Synchronization with Monitor"
lock := Monitor new.
lock critical: [
  sharedCounter := sharedCounter + 1
]

"Producer-consumer with SharedQueue"
queue := SharedQueue new: 10.
Processor fork: [1 to: 100 do: [:i | queue put: i]].
Processor fork: [100 timesRepeat: [Transcript show: queue take]]
```

### 3.8. Error Handling

**Process termination while holding locks:**
- Release all monitors held by the terminating process.
- Wake waiters with an error condition (ProcessTerminated).

**Uncaught exceptions in processes:**
- Terminate the process with state `psTerminated`.
- Log the error to a system transcript or error log.
- Other processes continue unaffected.

**Deadlock detection (optional enhancement):**
- Maintain a wait-for graph: edges from blocked process to the process holding what it waits on.
- Detect cycles in the graph to identify deadlocks.
- Report deadlock to debugger or system console.

---

## 4. Implementation Roadmap

### Phaseâ€¯1: Core Scheduler and Monitor (âœ… Completed)

- [x] `nimtalk/core/process.nim`: Process, Scheduler types.
- [x] Basic roundâ€‘robin scheduler, with only explicit yields.
- [ ] Monitor: Nimâ€‘side reâ€‘entrant lock + condition variable.
- [x] Simple yield and block/unblock operations.
- [x] `nimtalk/core/scheduler.nim`: Scheduler-Interpreter integration.
- [x] Process forking with shared globals and rootObject.
- [x] Processor global object with `yield`, `fork:`, `current` methods.
- [x] Test suite for scheduler and process lifecycle.

**Status**: Core scheduler is fully functional. Each process has its own interpreter with shared globals and rootObject. Round-robin scheduling with explicit yields is working.

### Phaseâ€¯2: Synchronisation Primitives (ðŸš§ In Progress)

- [ ] Semaphore (binary, counting).
- [ ] SharedQueue (bounded buffer) atop Monitor.
- [ ] Nemoâ€‘side `Monitor` and `SharedQueue` objects.
- [x] Process spawning from Nemo: `Processor fork: aBlock`.
- [ ] Debugger: inspection of process stack frames.

**Status**: `Processor fork:` and `Processor yield` are implemented. Monitors and SharedQueues are planned but not yet implemented.

### Phaseâ€¯3: GTK Bridge (ðŸ“‹ Planned)

- [ ] Basic GTK widget objects (Window, Button, Box, TextView).
- [ ] Eventâ€‘loop hook for GTK idles.
- [ ] Signalâ€‘toâ€‘callback mapping (GTK signal â†’ Nemo block evaluation).

### Phaseâ€¯4: Nemoâ€‘side UI Framework (ðŸ“‹ Planned)

- [ ] `GtkApplication` Nemoâ€‘side class.
- [ ] Example apps: Transcript, simple editor, process inspector.

### Phaseâ€¯5: Channels and Actorâ€‘style (ðŸ“‹ Future)

- [ ] Channels: bounded capacity, typed.
- [ ] Actorâ€‘mailbox processes.
- [ ] Supervisorâ€‘style processâ€‘linking (reâ€‘invent OTP for Nemo).

### Why Green Threads Before GTK?

Green threads are not strictly required for GTK4 integration. A simpler model could work:
- Single interpreter, no scheduler
- GTK events directly invoke Nemo blocks
- Each callback runs to completion before returning to GTK

This works for simple UIs but has limitations:
- Long-running Nemo code blocks the UI
- No background processing while UI is active
- No debugger that can inspect a running process

**Reasons to do green threads first:**

1. **The GTK integration design assumes the scheduler exists** â€“ the idle callback strategy, event-to-process routing, etc. all hook into scheduler infrastructure.

2. **Responsive UI requires it** â€“ even a simple "Cancel" button during a computation needs another process to handle the click.

3. **The debugger use case** â€“ one of the main goals is a Nemo debugger that can step through another process. This fundamentally requires multiple processes.

4. **Retrofitting is harder** â€“ adding green threads later means rewriting the GTK integration layer.

**Practical approach**: Implement a minimal Phase 1 (basic scheduler with explicit yields only, no monitors yet), then start Phase 3 GTK work in parallel with Phase 2. The core scheduler loop and process spawning are what GTK integration really needs â€“ the synchronization primitives can come later.

---

## 5. Open Questions / Design Decisions

- **Priority inversion** â€“ simple strict FIFO of same priority. Could add priority inheritance later.
- **Processâ€‘local GC heaps?** Not for now; singleâ€‘heap with sharing is fine for small objects.
- **Channel type safety** â€“ Nimâ€‘side generic channels possible; Nemo values carry typeâ€‘tags.

---

## 6. Why Monitors as Foundation?

1. **Familiar to Smalltalk programmers** (`Objectâ€‘â€‘MonitorStateâ€‘â€‘Processâ€‘â€‘Condition`).
2. **Reâ€‘entrant locks** are easier for Nemo: process can reâ€‘enter a Monitor it already holds.
3. **Condition variables** are exactly the synchronization primitive we need for blocking queues, semaphores, etc.
4. Simple blocking semantics fit the greenâ€‘thread scheduler perfectly.

---

## 7. Summary: What This Gives Us

A simple, **debuggable** cooperativeâ€‘concurrency system, where all UIâ€‘level constructs are just Nemo objects, fully inspectable and changeable at runtime.

The same cooperative scheduler that runs your GUI app can run the debugger that steps through it. The same green threads that handle UI events also run your application logic. All built from Monitors and oneâ€‘threadâ€‘atâ€‘aâ€‘time execution.

---

## API Reference

### Nemo-Side API

The `Processor` global object provides the main interface for green thread operations:

#### Processor yield
Yields the current process, allowing other ready processes to run.

```smalltalk
"Yield to other processes"
Processor yield
```

#### Processor fork: aBlock
Creates a new green process that will execute `aBlock` when scheduled.

```smalltalk
"Fork a new process"
process := Processor fork: [
  1 to: 10 do: [:i |
    Stdout writeline: i.
    Processor yield
  ]
]
```

#### Processor current
Returns the current process object (placeholder - returns nil in current implementation).

```smalltalk
"Get current process"
current := Processor current
```

### Nim-Side API

#### SchedulerContext
The main entry point for scheduler operations:

```nim
# Create scheduler with main process
let ctx = newSchedulerContext()

# Access scheduler and main process
let sched = ctx.scheduler
let main = ctx.mainProcess
```

#### Process Management

```nim
# Fork a new process from a block
let newProc = ctx.forkProcess(blockNode, receiver, "process-name")

# Run one time slice
let ran = ctx.runOneSlice()

# Run until all processes complete
let steps = ctx.runToCompletion(maxSteps = 10000)

# Run until no ready processes
let steps = ctx.runUntilIdle(maxSteps = 1000)
```

#### Process State

```nim
type ProcessState = enum
  psReady       # Ready to run
  psRunning     # Currently executing
  psBlocked     # Blocked on synchronization
  psSuspended   # Suspended for debugging
  psTerminated  # Finished execution
```

#### Scheduler Inspection

```nim
# Get process counts
let total = sched.processCount
let ready = sched.readyCount
let blocked = sched.blockedCount

# List all processes
for (pid, name, state) in sched.listProcesses():
  echo pid, " ", name, " ", state

# Print scheduler status
echo sched.printStatus()
```

---

**Plan approved** 2025â€‘01â€‘30.
**Phase 1 Completed** 2026â€‘01â€‘31 â€“ Core scheduler with explicit yields, process forking, and test suite.